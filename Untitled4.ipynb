{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11a1ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c20e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normilize the values\n",
    "x_train = x_train/255.00\n",
    "y_train = y_train/255.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487987d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models,layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4eb8a91",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'logging'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 92\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[38;5;241m.\u001b[39mset_verbosity(tf\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[1;32m     94\u001b[0m tf\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mDEFINE_string(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel name.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m tf\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mDEFINE_string(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset name.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'logging'"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import __future__\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "REMOTE_URL = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "LOCAL_DIR = os.path.join(\"data/cifar100/\")\n",
    "ARCHIVE_NAME = \"cifar-100-python.tar.gz\"\n",
    "DATA_DIR = \"cifar-100-python/\"\n",
    "TRAIN_BATCHES = [\"train\"]\n",
    "TEST_BATCHES = [\"test\"]\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CLASSES = 100\n",
    "\n",
    "def get_params():\n",
    "    \"\"\"Return dataset parameters.\"\"\"\n",
    "    return {\n",
    "        \"image_size\": IMAGE_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "    }\n",
    "\n",
    "def prepare():\n",
    "    \"\"\"Download the cifar 100 dataset.\"\"\"\n",
    "    if not os.path.exists(LOCAL_DIR):\n",
    "        os.makedirs(LOCAL_DIR)\n",
    "    if not os.path.exists(LOCAL_DIR + ARCHIVE_NAME):\n",
    "        print(\"Downloading...\")\n",
    "        urllib.request.urlretrieve(REMOTE_URL, LOCAL_DIR + ARCHIVE_NAME)\n",
    "    if not os.path.exists(LOCAL_DIR + DATA_DIR):\n",
    "        print(\"Extracting files...\")\n",
    "        tar = tarfile.open(LOCAL_DIR + ARCHIVE_NAME)\n",
    "        tar.extractall(LOCAL_DIR)\n",
    "        tar.close()\n",
    "\n",
    "def read(split):\n",
    "    \"\"\"Create an instance of the dataset object.\"\"\"\n",
    "    batches = {\n",
    "        tf.estimator.ModeKeys.TRAIN: TRAIN_BATCHES,\n",
    "        tf.estimator.ModeKeys.EVAL: TEST_BATCHES\n",
    "    }[split]\n",
    "\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in batches:\n",
    "        with open(\"%s%s%s\" % (LOCAL_DIR, DATA_DIR, batch), \"rb\") as fo:\n",
    "            dict = cPickle.load(fo)\n",
    "            images = np.array(dict[\"data\"])\n",
    "            labels = np.array(dict[\"fine_labels\"])\n",
    "\n",
    "            num = images.shape[0]\n",
    "            images = np.reshape(images, [num, 3, IMAGE_SIZE, IMAGE_SIZE])\n",
    "            images = np.transpose(images, [0, 2, 3, 1])\n",
    "            print(\"Loaded %d examples.\" % num)\n",
    "\n",
    "            all_images.append(images)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_images = np.concatenate(all_images)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    return tf.contrib.data.Dataset.from_tensor_slices((all_images, all_labels))\n",
    "\n",
    "def parse(image, label):\n",
    "    \"\"\"Parse input record to features and labels.\"\"\"\n",
    "    image = tf.to_float(image) / 255.0\n",
    "    image = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    return {\"image\": image}, {\"label\": label}\n",
    "\n",
    "\n",
    "\"\"\"This module handles training and evaluation of a neural network model.\n",
    "\n",
    "Invoke the following command to train the model:\n",
    "python -m trainer --model=cnn --dataset=mnist\n",
    "\n",
    "You can then monitor the logs on Tensorboard:\n",
    "tensorboard --logdir=output\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "tf.flags.DEFINE_string(\"model\", \"\", \"Model name.\")\n",
    "tf.flags.DEFINE_string(\"dataset\", \"\", \"Dataset name.\")\n",
    "tf.flags.DEFINE_string(\"output_dir\", \"\", \"Optional output dir.\")\n",
    "tf.flags.DEFINE_string(\"schedule\", \"train_and_evaluate\", \"Schedule.\")\n",
    "tf.flags.DEFINE_string(\"hparams\", \"\", \"Hyper parameters.\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 100000, \"Number of training epochs.\")\n",
    "tf.flags.DEFINE_integer(\"save_summary_steps\", 10, \"Summary steps.\")\n",
    "tf.flags.DEFINE_integer(\"save_checkpoints_steps\", 10, \"Checkpoint steps.\")\n",
    "tf.flags.DEFINE_integer(\"eval_steps\", None, \"Number of eval steps.\")\n",
    "tf.flags.DEFINE_integer(\"eval_frequency\", 10, \"Eval frequency.\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "MODELS = {\n",
    "    # This is a dictionary of models, the keys are model names, and the values\n",
    "    # are the module containing get_params, model, and eval_metrics.\n",
    "    # Example: \"cnn\": cnn\n",
    "}\n",
    "\n",
    "DATASETS = {\n",
    "    # This is a dictionary of datasets, the keys are dataset names, and the\n",
    "    # values are the module containing get_params, prepare, read, and parse.\n",
    "    # Example: \"mnist\": mnist\n",
    "}\n",
    "\n",
    "HPARAMS = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"decay_steps\": 10000,\n",
    "    \"batch_size\": 128\n",
    "}\n",
    "\n",
    "def get_params():\n",
    "    \"\"\"Aggregates and returns hyper parameters.\"\"\"\n",
    "    hparams = HPARAMS\n",
    "    hparams.update(DATASETS[FLAGS.dataset].get_params())\n",
    "    hparams.update(MODELS[FLAGS.model].get_params())\n",
    "\n",
    "    hparams = tf.contrib.training.HParams(**hparams)\n",
    "    hparams.parse(FLAGS.hparams)\n",
    "\n",
    "    return hparams\n",
    "\n",
    "def make_input_fn(mode, params):\n",
    "    \"\"\"Returns an input function to read the dataset.\"\"\"\n",
    "    def _input_fn():\n",
    "        dataset = DATASETS[FLAGS.dataset].read(mode)\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.repeat(FLAGS.num_epochs)\n",
    "            dataset = dataset.shuffle(params.batch_size * 5)\n",
    "        dataset = dataset.map(\n",
    "            DATASETS[FLAGS.dataset].parse, num_threads=8)\n",
    "        dataset = dataset.batch(params.batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        features, labels = iterator.get_next()\n",
    "        return features, labels\n",
    "    return _input_fn\n",
    "\n",
    "def make_model_fn():\n",
    "    \"\"\"Returns a model function.\"\"\"\n",
    "    def _model_fn(features, labels, mode, params):\n",
    "        model_fn = MODELS[FLAGS.model].model\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        predictions, loss = model_fn(features, labels, mode, params)\n",
    "\n",
    "        train_op = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            def _decay(learning_rate, global_step):\n",
    "                learning_rate = tf.train.exponential_decay(\n",
    "                    learning_rate, global_step, params.decay_steps, 0.5,\n",
    "                    staircase=True)\n",
    "                return learning_rate\n",
    "\n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss=loss,\n",
    "                global_step=global_step,\n",
    "                learning_rate=params.learning_rate,\n",
    "                optimizer=params.optimizer,\n",
    "                learning_rate_decay_fn=_decay)\n",
    "\n",
    "        return tf.contrib.learn.ModelFnOps(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=loss,\n",
    "            train_op=train_op)\n",
    "\n",
    "    return _model_fn\n",
    "\n",
    "def experiment_fn(run_config, hparams):\n",
    "    \"\"\"Constructs an experiment object.\"\"\"\n",
    "    estimator = tf.contrib.learn.Estimator(\n",
    "        model_fn=make_model_fn(), config=run_config, params=hparams)\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        estimator=estimator,\n",
    "        train_input_fn=make_input_fn(tf.estimator.ModeKeys.TRAIN, hparams),\n",
    "        eval_input_fn=make_input_fn(tf.estimator.ModeKeys.EVAL, hparams),\n",
    "        eval_metrics=MODELS[FLAGS.model].eval_metrics(hparams),\n",
    "        eval_steps=FLAGS.eval_steps,\n",
    "        min_eval_frequency=FLAGS.eval_frequency)\n",
    "\n",
    "def main(unused_argv):\n",
    "    \"\"\"Main entry point.\"\"\"\n",
    "    if FLAGS.output_dir:\n",
    "        model_dir = FLAGS.output_dir\n",
    "    else:\n",
    "        model_dir = \"output/%s_%s\" % (FLAGS.model, FLAGS.dataset)\n",
    "\n",
    "    DATASETS[FLAGS.dataset].prepare()\n",
    "\n",
    "    session_config = tf.ConfigProto()\n",
    "    session_config.allow_soft_placement = True\n",
    "    session_config.gpu_options.allow_growth = True\n",
    "    run_config = tf.contrib.learn.RunConfig(\n",
    "        model_dir=model_dir,\n",
    "        save_summary_steps=FLAGS.save_summary_steps,\n",
    "        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "        save_checkpoints_secs=None,\n",
    "        session_config=session_config)\n",
    "\n",
    "    tf.contrib.learn.learn_runner.run(\n",
    "        experiment_fn=experiment_fn,\n",
    "        run_config=run_config,\n",
    "        schedule=FLAGS.schedule,\n",
    "        hparams=get_params())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n",
    "\n",
    "\n",
    "features = tf.layers.dense(features, units=64, name=\"dense/1\")\n",
    "\n",
    "features = tf.layers.batch_normalization(features)\n",
    "\n",
    "\"\"\"MNIST dataset preprocessing and specifications.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "\n",
    "REMOTE_URL = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "LOCAL_DIR = \"data/mnist/\"\n",
    "TRAIN_IMAGE_URL = \"train-images-idx3-ubyte.gz\"\n",
    "TRAIN_LABEL_URL = \"train-labels-idx1-ubyte.gz\"\n",
    "TEST_IMAGE_URL = \"t10k-images-idx3-ubyte.gz\"\n",
    "TEST_LABEL_URL = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "def get_params():\n",
    "    \"\"\"Dataset params.\"\"\"\n",
    "    return {\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "    }\n",
    "\n",
    "def prepare():\n",
    "    \"\"\"This function will be called once to prepare the dataset.\"\"\"\n",
    "    if not os.path.exists(LOCAL_DIR):\n",
    "        os.makedirs(LOCAL_DIR)\n",
    "    for name in [\n",
    "            TRAIN_IMAGE_URL,\n",
    "            TRAIN_LABEL_URL,\n",
    "            TEST_IMAGE_URL,\n",
    "            TEST_LABEL_URL]:\n",
    "        if not os.path.exists(LOCAL_DIR + name):\n",
    "            urllib.request.urlretrieve(REMOTE_URL + name, LOCAL_DIR + name)\n",
    "\n",
    "def read(split):\n",
    "    \"\"\"Create an instance of the dataset object.\"\"\"\n",
    "    image_urls = {\n",
    "        tf.estimator.ModeKeys.TRAIN: TRAIN_IMAGE_URL,\n",
    "        tf.estimator.ModeKeys.EVAL: TEST_IMAGE_URL\n",
    "    }[split]\n",
    "    label_urls = {\n",
    "        tf.estimator.ModeKeys.TRAIN: TRAIN_LABEL_URL,\n",
    "        tf.estimator.ModeKeys.EVAL: TEST_LABEL_URL\n",
    "    }[split]\n",
    "\n",
    "    with gzip.open(LOCAL_DIR + image_urls, \"rb\") as f:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        images = np.frombuffer(f.read(num * rows * cols), dtype=np.uint8)\n",
    "        images = np.reshape(images, [num, rows, cols, 1])\n",
    "        print(\"Loaded %d images of size [%d, %d].\" % (num, rows, cols))\n",
    "\n",
    "    with gzip.open(LOCAL_DIR + label_urls, \"rb\") as f:\n",
    "        magic, num = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(num), dtype=np.int8)\n",
    "        print(\"Loaded %d labels.\" % num)\n",
    "\n",
    "    return tf.contrib.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "def parse(image, label):\n",
    "    \"\"\"Parse input record to features and labels.\"\"\"\n",
    "    image = tf.to_float(image) / 255.0\n",
    "    label = tf.to_int64(label)\n",
    "    return {\"image\": image}, {\"label\": label}\n",
    "\n",
    "\n",
    "keras.datasets.cifar100.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06328f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
